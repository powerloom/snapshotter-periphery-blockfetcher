Directory structure:
‚îî‚îÄ‚îÄ snapshotter-periphery-blockfetcher/
    ‚îú‚îÄ‚îÄ README.md
    ‚îú‚îÄ‚îÄ docker-compose.yaml
    ‚îú‚îÄ‚îÄ Dockerfile
    ‚îú‚îÄ‚îÄ main.py
    ‚îú‚îÄ‚îÄ pyproject.toml
    ‚îú‚îÄ‚îÄ .env.example
    ‚îú‚îÄ‚îÄ config/
    ‚îÇ   ‚îú‚îÄ‚îÄ loader.py
    ‚îÇ   ‚îú‚îÄ‚îÄ preloaders.json
    ‚îÇ   ‚îî‚îÄ‚îÄ settings.template.json
    ‚îú‚îÄ‚îÄ scripts/
    ‚îÇ   ‚îú‚îÄ‚îÄ entrypoint.py
    ‚îÇ   ‚îú‚îÄ‚îÄ generate_preloader_template.py
    ‚îÇ   ‚îî‚îÄ‚îÄ generate_settings_template.py
    ‚îî‚îÄ‚îÄ utils/
        ‚îú‚îÄ‚îÄ block_fetcher.py
        ‚îú‚îÄ‚îÄ exceptions.py
        ‚îú‚îÄ‚îÄ logging.py
        ‚îú‚îÄ‚îÄ models/
        ‚îÇ   ‚îú‚îÄ‚îÄ redis_keys.py
        ‚îÇ   ‚îî‚îÄ‚îÄ settings_model.py
        ‚îú‚îÄ‚îÄ preloaders/
        ‚îÇ   ‚îú‚îÄ‚îÄ base.py
        ‚îÇ   ‚îú‚îÄ‚îÄ block_details.py
        ‚îÇ   ‚îî‚îÄ‚îÄ manager.py
        ‚îî‚îÄ‚îÄ redis/
            ‚îú‚îÄ‚îÄ data_manager.py
            ‚îî‚îÄ‚îÄ redis_conn.py

================================================
FILE: README.md
================================================
# snapshotter-periphery-blockfetcher

## Local Testing Instructions

To set up and run the block fetcher service locally, follow these steps:

### Prerequisites

- Ensure you have Docker and Docker Compose installed on your machine.

### Setup

1. **Clone the Repository**:
   ```bash
   git clone <repository-url>
   cd snapshotter-periphery-blockfetcher
   ```

2. **Create a `.env` File**:
   - Copy the `.env.example` file to `.env` and fill in the necessary environment variables.
   ```bash
   cp .env.example .env
   ```

3. **Build and Start the Services**:
   - Use Docker Compose to build and start the services. The `local` profile will include the Redis service for local testing.
   ```bash
   docker-compose --profile local up --build
   ```

4. **Access Logs**:
   - Logs are available in the `./logs` directory. You can also view logs in real-time using:
   ```bash
   docker-compose logs -f
   ```

5. **Shut Down the Services**:
   - To stop the services, use:
   ```bash
   docker-compose down
   ```

### Notes

- The `local` profile includes a Redis service for testing purposes. In production, ensure that your configuration points to an external Redis service.
- Ensure that all necessary environment variables are set in the `.env` file for the application to function correctly.



================================================
FILE: docker-compose.yaml
================================================
version: '3.8'

services:
  block_fetcher:
    build: .
    environment:
      - RPC_URL
      - RPC_RATE_LIMIT
      - RPC_RETRY
      - RPC_TIMEOUT
      - POLLING_INTERVAL
      - REDIS_HOST
      - REDIS_PORT
      - REDIS_DB
      - REDIS_PASSWORD
      - REDIS_SSL
      - REDIS_CLUSTER
      - REDIS_MAX_BLOCKS
      - REDIS_TTL_SECONDS
      - LOG_DEBUG
      - LOG_TO_FILES
      - LOG_LEVEL
      - TEST_MODE=${TEST_MODE:-false}
    depends_on:
      - redis
    volumes:
      - ./logs:/app/logs
    env_file:
      - .env

  redis:
    image: "redis:7-alpine"
    command: redis-server --appendonly yes --port ${REDIS_PORT:-6379} --maxmemory ${REDIS_MAX_MEMORY:-512mb} --maxmemory-policy ${REDIS_MAXMEMORY_POLICY:-allkeys-lru}
    ports:
      - "${REDIS_PORT:-6379}:6379"
    healthcheck:
      test: ["CMD", "sh", "-c", "redis-cli -h ${REDIS_HOST:-localhost} -p ${REDIS_PORT:-6379} ping | grep -q PONG && ! redis-cli -h ${REDIS_HOST:-localhost} -p ${REDIS_PORT:-6379} info persistence | grep -q 'loading:1'"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 30s
    restart: on-failure
    volumes:
      - ./redis_data:/data
    # Add optional password and memory configurations
    environment:
      - REDIS_PASSWORD
      - REDIS_HOST
    profiles:
      - local
      - test

  rate-limiter:
    image: ${RATE_LIMITER_IMAGE:-ghcr.io/powerloom/rate-limiter:dockerify}
    environment:
      - DEFAULT_RATE_LIMIT=${DEFAULT_RATE_LIMIT:-1000}
    command: poetry run uvicorn app:app --host 0.0.0.0 --port ${RATE_LIMITER_PORT:-8000}
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:${RATE_LIMITER_PORT:-8000}/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    profiles:
      - local
      - test
    env_file:
      - .env



================================================
FILE: Dockerfile
================================================
FROM python:3.12-slim

# Install system dependencies
RUN apt-get update && apt-get install -y \
    gcc \
    && rm -rf /var/lib/apt/lists/*

# Set working directory
WORKDIR /app

# Install Poetry
RUN pip install poetry

# Copy dependency files
COPY pyproject.toml poetry.lock ./

# Configure Poetry to not create virtual environment in container
RUN poetry config virtualenvs.create false

# Install dependencies without installing the project itself
RUN poetry install --no-root --no-interaction

# Create logs directory
RUN mkdir logs

# Copy application code
COPY . .

# Ensure entrypoint script is executable
RUN chmod +x scripts/entrypoint.py

# Set environment variables
ENV PYTHONUNBUFFERED=1

# Use entrypoint script to start the service
CMD ["python", "scripts/entrypoint.py"]



================================================
FILE: main.py
================================================
import asyncio
import signal
import os
from config.loader import get_core_config
from utils.block_fetcher import BlockFetcher
from utils.logging import logger, configure_file_logging

class BlockProcessor:
    def __init__(self):
        self.settings = get_core_config()
        self.block_fetcher = BlockFetcher()
        self.shutdown_event = asyncio.Event()
        self._logger = logger.bind(module='BlockProcessor')
        
        configure_file_logging(self.settings.logs.write_to_files)
        
    async def process_blocks(self):
        """Main processing loop for blocks."""
        try:
            # Check for test mode from environment
            test_mode = os.getenv('TEST_MODE', 'false').lower() == 'true'
            if test_mode:
                self._logger.info("üß™ Starting in test mode (will process one block and wait)")
                await self.block_fetcher.start_test_mode()
            else:
                await self.block_fetcher.start_continuous_processing(
                    poll_interval=self.settings.source_rpc.polling_interval
                )
        except Exception as e:
            self._logger.error(f"Error in block processing: {str(e)}")
            self.shutdown_event.set()

    def handle_shutdown(self, signum, frame):
        """Handle shutdown signals gracefully."""
        self._logger.info(f"Received signal {signum}. Shutting down gracefully...")
        self.shutdown_event.set()

async def main():
    processor = BlockProcessor()
    
    # Set up signal handlers
    for sig in (signal.SIGTERM, signal.SIGINT):
        signal.signal(sig, processor.handle_shutdown)
    
    # Start the block processing
    try:
        await processor.process_blocks()
    except Exception as e:
        logger.error(f"Fatal error: {str(e)}")
    finally:
        logger.info("Shutdown complete.")

if __name__ == "__main__":
    asyncio.run(main())




================================================
FILE: pyproject.toml
================================================
[tool.poetry]
name = "SnapshotterBlockFetcher"
version = "0.1.0"
description = "Block Fetch Entry Point. Fetches blocks and pushes them downstream to block unpackers."
authors = ["Anomit Ghosh <anomit@powerloom.io>"]
license = "GPL v3"
readme = "README.md"
package-mode = false

[tool.poetry.dependencies]
python = "^3.12"
web3 = "^6.17.1"
httpx = "^0.27.1"
tenacity = "^9.1.2"
aiolimiter = "^1.2.1"
pydantic = "^2.11.3"
loguru = "^0.7.3"
redis = "^5.2.1"
python-dotenv = "^1.1.0"
rpc_helper = { git = "https://github.com/PowerLoom/rpc-helper.git"}


[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"



================================================
FILE: .env.example
================================================
# Source RPC (Ethereum Mainnet) Configuration
SOURCE_RPC_URL=https://eth-mainnet.example.com
SOURCE_RPC_TIMEOUT=30
SOURCE_RPC_RETRY=3
SOURCE_RPC_RATE_LIMIT=100
SOURCE_RPC_POLLING_INTERVAL=5

# Redis Configuration
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DB=0
REDIS_PASSWORD=
REDIS_SSL=false
REDIS_CLUSTER=false
REDIS_MAX_BLOCKS=1000
REDIS_TTL_SECONDS=86400

# Logging Configuration
LOG_DEBUG=false
LOG_TO_FILES=true
LOG_LEVEL=INFO

# HTTPX Configuration
HTTPX_POOL_TIMEOUT=30
HTTPX_CONNECT_TIMEOUT=30
HTTPX_READ_TIMEOUT=30
HTTPX_WRITE_TIMEOUT=30

# Reporting Configuration
REPORTING_SLACK_URL=
REPORTING_SERVICE_URL=
REPORTING_TELEGRAM_URL=
REPORTING_TELEGRAM_CHAT_ID=
REPORTING_MIN_INTERVAL=300

# Additional Configuration
NAMESPACE=UNISWAPV2

# Rate Limiter Configuration
RATE_LIMITER_IMAGE=rate-limiter
RATE_LIMITER_PORT=8000
DEFAULT_RATE_LIMIT=1000



================================================
FILE: config/loader.py
================================================
import os
import json
from functools import lru_cache
from pathlib import Path
from utils.models.settings_model import Settings, PreloaderConfig
from utils.logging import logger

_logger = logger.bind(module='ConfigLoader')

SETTINGS_FILE = os.path.join(os.path.dirname(__file__), 'settings.json')
PRELOADER_CONFIG_FILE = os.path.join(os.path.dirname(__file__), 'preloaders.json')

@lru_cache()
def get_core_config() -> Settings:
    """Load settings from the settings.json file."""
    _logger.info(f"üìñ Loading settings from: {SETTINGS_FILE}")
    if not os.path.exists(SETTINGS_FILE):
        _logger.error(f"‚ùå Settings file not found at {SETTINGS_FILE}")
        raise RuntimeError(f"Settings file not found at {SETTINGS_FILE}. Ensure the entrypoint script has run.")
    try:
        with open(SETTINGS_FILE, 'r') as f:
            settings_dict = json.load(f)
        settings = Settings(**settings_dict)
        _logger.success("‚úÖ Successfully loaded settings")
        return settings
    except json.JSONDecodeError as e:
        _logger.error(f"‚ùå Error decoding settings file: {e}")
        raise RuntimeError(f"Error decoding settings file ({SETTINGS_FILE}): {str(e)}")
    except Exception as e:
        _logger.error(f"‚ùå Error loading settings: {e}")
        raise RuntimeError(f"Error loading settings from {SETTINGS_FILE}: {str(e)}")

@lru_cache()
def get_preloader_config() -> PreloaderConfig:
    """Load preloader configuration from the preloaders.json file."""
    _logger.info(f"üìñ Loading preloader config from: {PRELOADER_CONFIG_FILE}")
    if not os.path.exists(PRELOADER_CONFIG_FILE):
        _logger.error(f"‚ùå Preloader config file not found")
        raise RuntimeError(f"Preloader config file not found at {PRELOADER_CONFIG_FILE}.")
    try:
        with open(PRELOADER_CONFIG_FILE, 'r') as f:
            config_dict = json.load(f)
        config = PreloaderConfig(**config_dict)
        _logger.success(f"‚úÖ Successfully loaded {len(config.preloaders)} preloader configurations")
        return config
    except json.JSONDecodeError as e:
        _logger.error(f"‚ùå Error decoding preloader config: {e}")
        raise RuntimeError(f"Error decoding preloader config file ({PRELOADER_CONFIG_FILE}): {str(e)}")
    except Exception as e:
        _logger.error(f"‚ùå Error loading preloader config: {e}")
        raise RuntimeError(f"Error loading preloader config from {PRELOADER_CONFIG_FILE}: {str(e)}")




================================================
FILE: config/preloaders.json
================================================
{
    "preloaders": [
        {
            "task_type": "block_details",
            "module": "utils.preloaders.block_details",
            "class_name": "BlockDetailsDumper"
        }
    ]
}



================================================
FILE: config/settings.template.json
================================================
{
  "namespace": "${NAMESPACE}",
  "source_rpc": {
    "full_nodes": [
      {
        "url": "${SOURCE_RPC_URL}",
        "rate_limit": {
          "requests_per_second": "${SOURCE_RPC_RATE_LIMIT}"
        }
      }
    ],
    "archive_nodes": null,
    "force_archive_blocks": null,
    "retry": "${SOURCE_RPC_RETRY}",
    "request_time_out": "${SOURCE_RPC_TIMEOUT}",
    "connection_limits": {
      "max_connections": 100,
      "max_keepalive_connections": 50,
      "keepalive_expiry": 300
    },
    "polling_interval": "${SOURCE_RPC_POLLING_INTERVAL}",
    "semaphore_value": 20
  },
  "redis": {
    "host": "${REDIS_HOST}",
    "port": "${REDIS_PORT}",
    "db": "${REDIS_DB}",
    "password": "${REDIS_PASSWORD}",
    "ssl": "${REDIS_SSL}",
    "cluster_mode": "${REDIS_CLUSTER}",
    "data_retention": {
      "max_blocks": "${REDIS_MAX_BLOCKS}",
      "ttl_seconds": "${REDIS_TTL_SECONDS}",
      "max_timestamps": "${REDIS_MAX_TIMESTAMPS}"
    }
  },
  "logs": {
    "debug_mode": "${LOG_DEBUG}",
    "write_to_files": "${LOG_TO_FILES}",
    "level": "${LOG_LEVEL}"
  },
  "httpx": {
    "pool_timeout": "${HTTPX_POOL_TIMEOUT}",
    "connect_timeout": "${HTTPX_CONNECT_TIMEOUT}",
    "read_timeout": "${HTTPX_READ_TIMEOUT}",
    "write_timeout": "${HTTPX_WRITE_TIMEOUT}"
  },
  "reporting": {
    "slack_url": "${REPORTING_SLACK_URL}",
    "service_url": "${REPORTING_SERVICE_URL}",
    "telegram_url": "${REPORTING_TELEGRAM_URL}",
    "telegram_chat_id": "${REPORTING_TELEGRAM_CHAT_ID}",
    "min_reporting_interval": "${REPORTING_MIN_INTERVAL}"
  }
}


================================================
FILE: scripts/entrypoint.py
================================================
#!/usr/bin/env python3
import os
import json
import subprocess
from string import Template
from dotenv import load_dotenv

def fill_template():
    """Fill settings template with environment variables"""
    # Load environment variables from .env file
    load_dotenv()

    template_path = 'config/settings.template.json'
    if not os.path.exists(template_path):
        print(f"Template file not found: {template_path}")
        return

    with open(template_path, 'r') as f:
        template = Template(f.read())
    
    # Debug: Print environment variables
    print("Environment Variables:")
    for key, value in os.environ.items():
        print(f"{key}={value}")

    # Get env vars with defaults
    filled = template.safe_substitute(os.environ)
    
    # Debug: Print filled template
    print("Filled Template:")
    print(filled)

    # Write filled settings
    with open('config/settings.json', 'w') as f:
        f.write(filled)

if __name__ == "__main__":
    fill_template()
    # Execute main application
    subprocess.run(["python", "main.py"]) 


================================================
FILE: scripts/generate_preloader_template.py
================================================
import json
from pathlib import Path

def generate_template():
    """Generate template preloaders.json with default configuration."""
    template = {
        "preloaders": [
            {
                "task_type": "block_details",
                "module": "utils.preloaders.block_details",
                "class_name": "BlockDetailsDumper"
            }
        ]
    }

    config_dir = Path(__file__).parent.parent / 'config'
    config_dir.mkdir(exist_ok=True)
    template_path = config_dir / 'preloaders.json'

    with open(template_path, 'w') as f:
        json.dump(template, f, indent=2)
    print(f"Generated preloader template at {template_path}")

if __name__ == "__main__":
    generate_template()



================================================
FILE: scripts/generate_settings_template.py
================================================
import json

def generate_template():
    """Generate template settings.json with placeholder values"""
    template = {
        "namespace": "${NAMESPACE}",
        "source_rpc": {
            "full_nodes": [
                {
                    "url": "${SOURCE_RPC_URL}",
                    "rate_limit": {
                        "requests_per_second": "${SOURCE_RPC_RATE_LIMIT}"
                    }
                }
            ],
            "archive_nodes": None,
            "force_archive_blocks": None,
            "retry": "${SOURCE_RPC_RETRY}",
            "request_time_out": "${SOURCE_RPC_TIMEOUT}",
            "connection_limits": {
                "max_connections": 100,
                "max_keepalive_connections": 50,
                "keepalive_expiry": 300
            },
            "polling_interval": "${SOURCE_RPC_POLLING_INTERVAL}",
            "semaphore_value": 20
        },
        "redis": {
            "host": "${REDIS_HOST}",
            "port": "${REDIS_PORT}",
            "db": "${REDIS_DB}",
            "password": "${REDIS_PASSWORD}",
            "ssl": "${REDIS_SSL}",
            "cluster_mode": "${REDIS_CLUSTER}",
            "data_retention": {
                "max_blocks": "${REDIS_MAX_BLOCKS}",
                "ttl_seconds": "${REDIS_TTL_SECONDS}",
                "max_timestamps": "${REDIS_MAX_TIMESTAMPS}"
            }
        },
        "logs": {
            "debug_mode": "${LOG_DEBUG}",
            "write_to_files": "${LOG_TO_FILES}",
            "level": "${LOG_LEVEL}"
        },
        "httpx": {
            "pool_timeout": "${HTTPX_POOL_TIMEOUT}",
            "connect_timeout": "${HTTPX_CONNECT_TIMEOUT}",
            "read_timeout": "${HTTPX_READ_TIMEOUT}",
            "write_timeout": "${HTTPX_WRITE_TIMEOUT}"
        },
        "reporting": {
            "slack_url": "${REPORTING_SLACK_URL}",
            "service_url": "${REPORTING_SERVICE_URL}",
            "telegram_url": "${REPORTING_TELEGRAM_URL}",
            "telegram_chat_id": "${REPORTING_TELEGRAM_CHAT_ID}",
            "min_reporting_interval": "${REPORTING_MIN_INTERVAL}"
        }
    }
    
    with open('config/settings.template.json', 'w') as f:
        json.dump(template, f, indent=2)

if __name__ == "__main__":
    generate_template()



================================================
FILE: utils/block_fetcher.py
================================================
from typing import List, Dict, Type
import asyncio
import json
import os
import redis.asyncio as aioredis
from datetime import datetime
from config.loader import get_core_config, get_preloader_config, PRELOADER_CONFIG_FILE
from rpc_helper.rpc import RpcHelper
from utils.logging import logger
from utils.redis.redis_conn import RedisPool
from utils.models.redis_keys import block_cache_key, block_tx_htable_key
from utils.preloaders.manager import PreloaderManager
from utils.preloaders.base import PreloaderHook
from utils.preloaders.block_details import BlockDetailsDumper
import time

settings = get_core_config()

class BlockFetcher:
    MAX_BLOCK_DIFFERENCE = 10  # Maximum allowed difference from head
    _redis: aioredis.Redis

    def __init__(self):
        self.settings = get_core_config()
        self.rpc_helper = RpcHelper(self.settings.source_rpc)
        self.state_file = "block_fetcher_state.json"
        self.last_processed_block = self._load_state()
        self._logger = logger.bind(module='BlockFetcher')
        self.tx_queue_key = f'pending_transactions:{self.settings.namespace}'
        self.block_cache_key = block_cache_key(self.settings.namespace)
        
        # Load preloader hooks from config
        self._logger.info(f"üîß Initializing BlockFetcher with namespace: {self.settings.namespace}")
        self._logger.info(f"üìã Using Redis queue key: {self.tx_queue_key}")
        self._logger.info(f"üíæ Using block cache key: {self.block_cache_key}")
        self._logger.info(f"üìÅ Loading preloader config from: {os.path.abspath(PRELOADER_CONFIG_FILE)}")
        preloader_config = get_preloader_config()
        self.preloader_hooks = PreloaderManager.load_hooks(preloader_config)
        self._logger.info(f"üîå Loaded {len(self.preloader_hooks)} preloader hooks:")
        for hook in self.preloader_hooks:
            self._logger.info(f"  ‚îú‚îÄ {hook.__class__.__name__}")
        self._initialized = False

    def _load_state(self) -> int:
        """Load the last processed block number from local file."""
        try:
            if os.path.exists(self.state_file):
                with open(self.state_file, 'r') as f:
                    state = json.load(f)
                    return state.get('last_processed_block', 0)
            return 0
        except Exception as e:
            self._logger.error(f"Error loading state from file: {str(e)}")
            return 0

    def _save_state(self, block_number: int):
        """Save the last processed block number to local file."""
        try:
            state = {'last_processed_block': block_number}
            with open(self.state_file, 'w') as f:
                json.dump(state, f)
            self._logger.debug(f"Saved last processed block {block_number} to {self.state_file}")
        except Exception as e:
            self._logger.error(f"Error saving state to file: {str(e)}")

    def extract_transaction_hashes(self, block: Dict) -> List[str]:
        """Extract transaction hashes from a block."""
        if not block or 'transactions' not in block:
            return []
        return block['transactions']

    def format_block_details(self, block: Dict) -> Dict:
        """Format block details for caching."""
        return {
            'timestamp': int(block.get('timestamp', '0x0'), 16),
            'number': int(block.get('number', '0x0'), 16),
            'transactions': block.get('transactions', []),
            'hash': block.get('hash'),
            'parentHash': block.get('parentHash'),
        }

    async def fetch_blocks_range(self, start_block: int, end_block: int) -> List[tuple[int, List[str]]]:
        """Fetch a range of blocks and extract their transaction hashes."""
        try:
            blocks = await self.rpc_helper.batch_eth_get_block(start_block, end_block)
            if not blocks:
                return []
            
            results = []
            for i, block in enumerate(blocks):
                if block and 'result' in block:
                    block_data = block['result']
                    tx_hashes = self.extract_transaction_hashes(block_data)
                    block_num = start_block + i
                    results.append((block_num, tx_hashes))
                    
                    # Run all preloader hooks
                    for hook in self.preloader_hooks:
                        try:
                            await hook.process_block(block_data, self.settings.namespace)
                        except Exception as e:
                            self._logger.error(f"Error in preloader hook {hook.__class__.__name__}: {e}")
            
            return results
        except Exception as e:
            self._logger.error(f"Error fetching blocks range {start_block}-{end_block}: {str(e)}")
            return []

    async def _init(self):
        """Initialize RPC and Redis connections."""
        if not self._initialized:
            await self.rpc_helper.init()
            self._redis = RedisPool.get_pool()
            asyncio.create_task(self._cleanup_expired_project_data())

            # Initialize preloader hooks
            for hook in self.preloader_hooks:
                if hasattr(hook, 'init'):
                    await hook.init()
            self._initialized = True

    async def close(self):
        """Cleanup resources."""
        # Cleanup preloader hooks
        for hook in self.preloader_hooks:
            if hasattr(hook, 'close'):
                await hook.close()
        
        await RedisPool.close()

    async def process_new_blocks(self) -> List[tuple[int, List[str]]]:
        """Process new blocks since last processed block."""
        try:
            await self._init()
            
            latest_block = await self.rpc_helper.get_current_block_number()
            
            # Check if we're too far behind
            block_difference = latest_block - self.last_processed_block
            if block_difference > self.MAX_BLOCK_DIFFERENCE:
                self._logger.warning(
                    f"üö® Too far behind head. Last processed: {self.last_processed_block}, "
                    f"Current head: {latest_block}, Difference: {block_difference}. "
                    f"Starting from {latest_block - self.MAX_BLOCK_DIFFERENCE}"
                )
                self._logger.info(
                    f"‚è≠Ô∏è Skipping blocks {self.last_processed_block + 1} to "
                    f"{latest_block - self.MAX_BLOCK_DIFFERENCE - 1}"
                )
                self.last_processed_block = latest_block - self.MAX_BLOCK_DIFFERENCE
                self._save_state(self.last_processed_block)

            if latest_block <= self.last_processed_block:
                self._logger.debug(
                    f"‚è∏Ô∏è No new blocks to process. Last processed: {self.last_processed_block}, "
                    f"Current head: {latest_block}"
                )
                return []

            # Process all blocks up to latest
            results = await self.fetch_blocks_range(self.last_processed_block + 1, latest_block)
            if results:
                # Push tx hashes to Redis queue
                for (block_number, tx_hashes) in results:
                    if tx_hashes:
                        try:
                            pushed_count = await self._redis.lpush(self.tx_queue_key, *tx_hashes)
                            self._logger.info(
                                f"üì¶ Block {block_number}: Pushed {pushed_count} tx hashes to queue '{self.tx_queue_key}' "
                                f"(first few: {', '.join(tx_hashes[:3])}{'...' if len(tx_hashes) > 3 else ''})"
                            )
                        except Exception as e:
                            self._logger.error(f"Error pushing tx hashes to Redis queue: {str(e)}")
                            continue
                self.last_processed_block = latest_block
                self._save_state(self.last_processed_block)
            
            return results
        except Exception as e:
            self._logger.error(f"üí• Error processing new blocks: {str(e)}")
            return []

    async def start_continuous_processing(self, poll_interval: float = 0.1):
        """Continuously process new blocks."""
        try:
            await self._init()
            
            while True:
                try:
                    results = await self.process_new_blocks()
                    for block_number, tx_hashes in results:
                        self._logger.info(f"‚õèÔ∏è Processed block {block_number} with {len(tx_hashes)} transactions")
                    if results:
                        pipeline = self._redis.pipeline()

                        for block_number, _ in results:
                            # Add to expiry tracking sorted set with TTL
                            expiry_time = int(time.time()) + 300
                            expiry_key = block_tx_htable_key(self.settings.namespace, block_number)
                            pipeline.zadd(
                                name=f'DataExpiry:BlockFetcher:{self.settings.namespace}',
                                mapping={expiry_key: expiry_time}
                            )
                        await pipeline.execute()
                    await asyncio.sleep(poll_interval)
                except Exception as e:
                    self._logger.error(f"üî• Error in continuous processing: {str(e)}")
                    await asyncio.sleep(poll_interval)
        finally:
            await self.close()

    async def _cleanup_expired_project_data(self):
        """
        Periodically checks for and removes expired project data entries.

        This method runs in the background to clean up project data hash entries
        that have exceeded their TTL as recorded in the expiry sorted set.
        """
        while True:
            try:
                self._logger.info(f"Cleaning up expired project data at {time.strftime('%Y-%m-%d %H:%M:%S')}")
                current_time = int(time.time())
                # Get all entries that have expired
                expired_entries = await self._redis.zrangebyscore(
                    f'DataExpiry:BlockFetcher:{self.settings.namespace}',
                    0,
                    current_time,
                    withscores=True
                )

                self._logger.info(f"Cleaning up {len(expired_entries)} expired project data entries")
                if expired_entries:                    
                    # Remove the entries from the hashmaps and the expiry set
                    pipeline = self._redis.pipeline()
                    for key, _ in expired_entries:
                        pipeline.delete(key)

                    # Remove from expiry tracking
                    pipeline.zrem(f'DataExpiry:BlockFetcher:{self.settings.namespace}', *[entry for entry, _ in expired_entries])

                    await pipeline.execute()

            except Exception as e:
                self._logger.error(f"Error cleaning up expired project data: {e}")

            self._logger.info(f"Sleeping for 60 seconds before next cleanup cycle")
            await asyncio.sleep(60)

    async def start_test_mode(self):
        """Process one block and then wait indefinitely."""
        await self.rpc_helper.init()
        self._logger.info("üß™ Starting BlockFetcher in test mode")
        
        try:
            results = await self.process_new_blocks()
            if results:
                for block_number, tx_hashes in results:
                    self._logger.info(f"‚õèÔ∏è Test mode: Processed block {block_number} with {len(tx_hashes)} transactions")
                    self._logger.info(f"üìä Transaction hashes: {', '.join(tx_hashes) if tx_hashes else 'None'}")
            else:
                self._logger.info("‚ÑπÔ∏è No new blocks to process in test mode")
            
            self._logger.info("üß™ Test mode: Waiting indefinitely...")
            # Wait indefinitely
            await asyncio.Event().wait()
            
        except Exception as e:
            self._logger.error(f"üî• Error in test mode: {str(e)}")
            raise 


================================================
FILE: utils/exceptions.py
================================================
import json


class SelfExitException(Exception):
    """
    Exception used by process hub core to signal core exit.

    This exception is raised when the process hub core needs to indicate
    that it should exit gracefully.
    """
    pass


class GenericExitOnSignal(Exception):
    """
    Exception to be used when a process or callback worker receives an exit signal.

    This exception is raised when a launched process or callback worker
    receives a signal to 'exit', such as SIGINT, SIGTERM, or SIGQUIT.
    """
    pass


class RPCException(Exception):
    """
    Exception class for handling RPC (Remote Procedure Call) related errors.

    This exception encapsulates information about an RPC error, including
    the original request, the response received, the underlying exception,
    and any additional information.
    """

    def __init__(self, request, response, underlying_exception, extra_info):
        """
        Initialize a new instance of the RPCException class.

        :param request: The request that caused the exception.
        :type request: Any
        :param response: The response received from the server.
        :type response: Any
        :param underlying_exception: The underlying exception that caused this exception.
        :type underlying_exception: Exception
        :param extra_info: Additional information about the exception.
        :type extra_info: Any
        """
        self.request = request
        self.response = response
        self.underlying_exception: Exception = underlying_exception
        self.extra_info = extra_info

    def __str__(self):
        """
        Return a JSON string representation of the exception object.

        :return: A JSON-formatted string containing exception details.
        :rtype: str
        """
        # Create a dictionary with exception details
        ret = {
            'request': self.request,
            'response': self.response,
            'extra_info': self.extra_info,
            'exception': None,
        }
        # Include the underlying exception if it exists
        if isinstance(self.underlying_exception, Exception):
            ret.update({'exception': str(self.underlying_exception)})
        return json.dumps(ret)

    def __repr__(self):
        """
        Return a string representation of the exception.

        This method returns the same output as __str__ for consistency.

        :return: A string representation of the exception.
        :rtype: str
        """
        return self.__str__()



================================================
FILE: utils/logging.py
================================================
import os
import sys
from pathlib import Path
from loguru import logger

# Create logs directory if it doesn't exist
LOGS_DIR = Path("logs")
LOGS_DIR.mkdir(exist_ok=True)

# Remove default logger
logger.remove()

# Define severity levels and their corresponding files
SEVERITY_FILES = {
    "ERROR": "error.log",
    "WARNING": "warning.log",
    "CRITICAL": "critical.log",
    "INFO": "info.log",
    "DEBUG": "debug.log",
    "TRACE": "trace.log",
    "SUCCESS": "success.log"
}

# Common log format for files
FILE_FORMAT = "{time:YYYY-MM-DD HH:mm:ss} | {level: <8} | {name}:{function}:{line} - {message}"

# Common log format for console (with colors)
CONSOLE_FORMAT = "<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>"

# Add console loggers for different severities
logger.add(
    sys.stdout,
    format=CONSOLE_FORMAT,
    level="INFO",
    colorize=True,
    filter=lambda record: record["level"].no < logger.level("WARNING").no
)

logger.add(
    sys.stderr,
    format=CONSOLE_FORMAT,
    level="WARNING",
    colorize=True,
    filter=lambda record: record["level"].no >= logger.level("WARNING").no
)

def configure_file_logging(write_to_files: bool = True):
    """Configure file-based logging based on settings."""
    if write_to_files:
        # Add file loggers for each severity level
        for level, filename in SEVERITY_FILES.items():
            logger.add(
                LOGS_DIR / filename,
                rotation="100 MB",
                retention="7 days",
                compression="zip",
                format=FILE_FORMAT,
                level=level,
                backtrace=True,
                diagnose=True,
                filter=lambda record, level=level: record["level"].name == level
            )

# Export the configured logger
__all__ = ["logger", "configure_file_logging"]



================================================
FILE: utils/models/redis_keys.py
================================================
def get_preloader_config_key() -> str:
    """Get the preloader config key."""
    return "preloader_config"

def block_cache_key(namespace: str) -> str:
    """Key for sorted set storing cached block details."""
    return f'block_cache:{namespace}'

def block_tx_htable_key(namespace: str, block_number: int) -> str:
    return f'block_txs:{block_number}:{namespace}'

def event_detector_last_processed_block(namespace: str) -> str:
    return f'SystemEventDetector:lastProcessedBlock:{namespace}'

def block_number_to_timestamp_key(namespace: str) -> str:
    return f'blockNumberToTimestamp:{namespace}'

def timestamp_to_block_number_key(namespace: str) -> str:
    return f'timestampToBlockNumber:{namespace}'



================================================
FILE: utils/models/settings_model.py
================================================
from pydantic import BaseModel
from rpc_helper.utils.models.settings_model import RPCConfigFull
from typing import List, Union

class HTTPXConfig(BaseModel):
    """HTTPX client configuration model."""
    pool_timeout: int
    connect_timeout: int
    read_timeout: int
    write_timeout: int

class RateLimitConfig(BaseModel):
    """RPC Rate limit configuration model."""
    requests_per_second: int


class Timeouts(BaseModel):
    """Timeout configuration model."""
    basic: int
    archival: int
    connection_init: int


class QueueConfig(BaseModel):
    """Queue configuration model."""
    num_instances: int


class ReportingConfig(BaseModel):
    """Reporting configuration model."""
    slack_url: str
    service_url: str
    telegram_url: str
    telegram_chat_id: str
    min_reporting_interval: int


class RedisDataRetentionConfig(BaseModel):
    """Redis data retention configuration model."""
    max_blocks: int  # Maximum number of blocks to keep in zsets
    max_timestamps: int  # Maximum number of block timestamps to keep in zsets
    ttl_seconds: int  # Default TTL for key-value pairs (24 hours)


class Redis(BaseModel):
    """Redis configuration model."""
    host: str
    port: int
    db: int
    password: Union[str, None] = None
    ssl: bool = False
    cluster_mode: bool = False
    data_retention: RedisDataRetentionConfig


class Preloader(BaseModel):
    """Preloader configuration model."""
    task_type: str
    module: str
    class_name: str

class PreloaderConfig(BaseModel):
    """Preloader configuration model."""
    preloaders: List[Preloader]

class Logs(BaseModel):
    """Logging configuration model."""
    debug_mode: bool
    write_to_files: bool


class Settings(BaseModel):
    """Main settings configuration model."""
    namespace: str
    source_rpc: RPCConfigFull
    httpx: HTTPXConfig
    reporting: ReportingConfig
    redis: Redis
    logs: Logs


================================================
FILE: utils/preloaders/base.py
================================================
from abc import ABC, abstractmethod
from typing import Dict, Any

class PreloaderHook(ABC):
    """Base class for preloader hooks."""
    
    async def init(self) -> None:
        """Initialize the preloader hook."""
        pass
    
    @abstractmethod
    async def process_block(self, block_data: Dict[str, Any], namespace: str) -> None:
        """Process a block."""
        pass
    
    async def close(self) -> None:
        """Cleanup resources."""
        pass



================================================
FILE: utils/preloaders/block_details.py
================================================
import json
from typing import Dict, Any
from .base import PreloaderHook
from utils.redis.data_manager import RedisDataManager
from utils.models.redis_keys import block_cache_key, block_number_to_timestamp_key, timestamp_to_block_number_key
from config.loader import get_core_config

class BlockDetailsDumper(PreloaderHook):
    """Dumps block details to Redis with data retention and pipelining."""
    
    def __init__(self):
        self.settings = get_core_config()
        self.data_manager = RedisDataManager(self.settings.redis.data_retention)
        # Bind logger after data_manager is initialized, if it has its own logger setup
        self._logger = self.data_manager._logger.bind(child_module='BlockDetailsDumper') 
    
    async def init(self):
        """Initialize the data manager."""
        await self.data_manager.init()
    
    async def process_block(self, block_data: Dict[str, Any], namespace: str) -> None:
        block_num = int(block_data['number'], 16)
        timestamp = int(block_data['timestamp'], 16) if block_data.get('timestamp') else None

        pipeline = await self.data_manager.get_pipeline()
        if not pipeline:
            self._logger.error("Failed to get Redis pipeline. Aborting block processing.")
            return

        potential_block_data_cleanup_score = None
        potential_ts_bnt_cleanup_score = None
        potential_ts_tnb_cleanup_score = None

        try:
            # Add full block data
            potential_block_data_cleanup_score = await self.data_manager.add_block_data_to_zset(
                block_cache_key(namespace),
                json.dumps(block_data),
                block_num,
                pipeline=pipeline
            )

            if timestamp is not None:
                # Add to block_number_to_timestamp_key ZSET (Score: block_num, Value: timestamp)
                potential_ts_bnt_cleanup_score = await self.data_manager.add_timestamp_by_block_num_zset(
                    block_number_to_timestamp_key(namespace),
                    json.dumps(timestamp),
                    block_num,
                    pipeline=pipeline
                )
                
                # Add to timestamp_to_block_number_key ZSET (Score: timestamp, Value: block_num)
                potential_ts_tnb_cleanup_score = await self.data_manager.add_block_num_by_timestamp_zset(
                    timestamp_to_block_number_key(namespace),
                    str(block_num),
                    timestamp,
                    pipeline=pipeline
                )
            else:
                self._logger.warning(f"Timestamp not found in block_data for block {block_num}. Only full block data ZSET updated.")

            await pipeline.execute()
            self._logger.debug(f"Successfully executed Redis pipeline for block {block_num}.")

            # Update last cleanup scores in the manager after successful execution
            await self.data_manager.update_last_cleanup_scores(
                block_data_score=potential_block_data_cleanup_score,
                ts_bnt_score=potential_ts_bnt_cleanup_score,
                ts_tnb_score=potential_ts_tnb_cleanup_score
            )

        except Exception as e:
            self._logger.error(f"Error during Redis pipeline execution or setup for block {block_num}: {e}", exc_info=True)

    
    async def close(self):
        """Cleanup resources."""
        await self.data_manager.close()



================================================
FILE: utils/preloaders/manager.py
================================================
import importlib
from typing import List, Type
from utils.models.settings_model import PreloaderConfig, Preloader
from utils.logging import logger
from .base import PreloaderHook

class PreloaderManager:
    """Manages preloader hook initialization and loading."""
    _logger = logger.bind(module='PreloaderManager')

    @classmethod
    def load_hook(cls, preloader_config: Preloader) -> PreloaderHook:
        """Load a single preloader hook."""
        cls._logger.info(f"üì• Loading preloader hook: {preloader_config.class_name}")
        try:
            module = importlib.import_module(preloader_config.module)
            hook_class = getattr(module, preloader_config.class_name)
            hook = hook_class()
            cls._logger.success(f"‚úÖ Successfully loaded {preloader_config.class_name}")
            return hook
        except Exception as e:
            cls._logger.error(f"‚ùå Failed to load {preloader_config.class_name}: {e}")
            raise ValueError(f"Failed to load preloader hook {preloader_config.class_name}: {e}")

    @classmethod
    def load_hooks(cls, config: PreloaderConfig) -> List[PreloaderHook]:
        """Load all configured preloader hooks."""
        cls._logger.info(f"üì¶ Loading {len(config.preloaders)} preloader hooks")
        hooks = []
        for preloader in config.preloaders:
            hook = cls.load_hook(preloader)
            hooks.append(hook)
        return hooks



================================================
FILE: utils/redis/data_manager.py
================================================
from typing import Optional, Any, Union
import redis.asyncio as aioredis
from utils.logging import logger
from utils.models.settings_model import RedisDataRetentionConfig
from utils.redis.redis_conn import RedisPool

class RedisDataManager:
    """Manages Redis data retention and cleanup."""
    
    def __init__(self, retention_config: RedisDataRetentionConfig):
        self.retention_config = retention_config
        self._redis: Optional[aioredis.Redis] = None
        self._logger = logger.bind(module='RedisDataManager')
        
        self._last_cleanup_block_data: int = 0
        self._last_cleanup_ts_bnt: int = 0 # For block_number_to_timestamp_key
        self._last_cleanup_ts_tnb: int = 0 # For timestamp_to_block_number_key
        
        # Cleanup intervals
        self._block_cleanup_interval = max(10, self.retention_config.max_blocks // 10)
        self._timestamp_cleanup_interval = max(10, self.retention_config.max_timestamps // 10)

        self._logger.info(
            f"Initialized RedisDataManager: "
            f"max_blocks={retention_config.max_blocks}, block_cleanup_interval={self._block_cleanup_interval}, "
            f"max_timestamps={retention_config.max_timestamps}, timestamp_cleanup_interval={self._timestamp_cleanup_interval}"
        )

    async def init(self):
        """Initialize Redis connection."""
        if not self._redis:
            self._redis = await RedisPool.get_pool()

    async def get_connection(self) -> Optional[aioredis.Redis]:
        """Returns the Redis connection, initializing if necessary."""
        if not self._redis:
            await self.init()
        return self._redis

    async def get_pipeline(self) -> Optional[aioredis.client.Pipeline]:
        """Returns a Redis pipeline, initializing connection if necessary."""
        conn = await self.get_connection()
        return conn.pipeline() if conn else None

    async def set_with_ttl(self, key: str, value: Any, ttl: Optional[int] = None, pipeline: Optional[aioredis.client.Pipeline] = None):
        """Set a key with TTL, optionally using a provided pipeline."""
        conn_or_pipe = pipeline if pipeline is not None else await self.get_connection()
        if not conn_or_pipe:
            self._logger.warning("Redis connection not available for set_with_ttl.")
            return

        ttl = ttl or self.retention_config.ttl_seconds
        await conn_or_pipe.set(key, value, ex=ttl)

    async def _add_to_zset_with_cleanup(
        self, 
        key: str, 
        value: str, 
        score: int, 
        last_cleanup_score: int, 
        cleanup_interval: int, 
        max_items: int,
        connection_or_pipeline: Union[aioredis.Redis, aioredis.client.Pipeline]
    ) -> int:
        """Helper to add to zset and maintain size limit, returns new last_cleanup_score."""
        # Add the new value
        await connection_or_pipeline.zadd(key, {value: score})
        
        new_last_cleanup_score = last_cleanup_score
        if score - last_cleanup_score >= cleanup_interval:
            min_score_to_keep = score - max_items 
            await connection_or_pipeline.zremrangebyscore(key, '-inf', min_score_to_keep)
            self._logger.debug(
                f"Cleanup for ZSET '{key}': zremrangebyscore '-inf' to {min_score_to_keep}. "
                f"Current score: {score}, last cleanup: {last_cleanup_score}, interval: {cleanup_interval}."
            )
            new_last_cleanup_score = score  # Update last_cleanup_score because cleanup happened
        return new_last_cleanup_score

    async def add_block_data_to_zset(self, key: str, value: str, score: int, pipeline: Optional[aioredis.client.Pipeline] = None) -> int:
        """Add block data to zset and maintain size limit based on max_blocks."""
        conn_or_pipe = pipeline if pipeline is not None else await self.get_connection()
        if not conn_or_pipe:
            self._logger.warning(f"Redis connection not available for add_block_data_to_zset on key {key}.")
            return self._last_cleanup_block_data 

        new_cleanup_score = await self._add_to_zset_with_cleanup(
            key=key,
            value=value,
            score=score,
            last_cleanup_score=self._last_cleanup_block_data,
            cleanup_interval=self._block_cleanup_interval,
            max_items=self.retention_config.max_blocks,
            connection_or_pipeline=conn_or_pipe
        )
        if pipeline is None: # Executed immediately
            self._last_cleanup_block_data = new_cleanup_score
        return new_cleanup_score

    async def add_timestamp_by_block_num_zset(self, key: str, value: str, score: int, pipeline: Optional[aioredis.client.Pipeline] = None) -> int:
        """Add to ZSET where score is block number, value is timestamp info. Uses max_timestamps config."""
        conn_or_pipe = pipeline if pipeline is not None else await self.get_connection()
        if not conn_or_pipe:
            self._logger.warning(f"Redis connection not available for add_timestamp_by_block_num_zset on key {key}.")
            return self._last_cleanup_ts_bnt

        new_cleanup_score = await self._add_to_zset_with_cleanup(
            key=key,
            value=value,
            score=score,
            last_cleanup_score=self._last_cleanup_ts_bnt,
            cleanup_interval=self._timestamp_cleanup_interval, # Using timestamp interval
            max_items=self.retention_config.max_timestamps,    # Using max_timestamps
            connection_or_pipeline=conn_or_pipe
        )
        if pipeline is None: # Executed immediately
            self._last_cleanup_ts_bnt = new_cleanup_score
        return new_cleanup_score

    async def add_block_num_by_timestamp_zset(self, key: str, value: str, score: int, pipeline: Optional[aioredis.client.Pipeline] = None) -> int:
        """Add to ZSET where score is timestamp, value is block number info. Uses max_timestamps config."""
        conn_or_pipe = pipeline if pipeline is not None else await self.get_connection()
        if not conn_or_pipe:
            self._logger.warning(f"Redis connection not available for add_block_num_by_timestamp_zset on key {key}.")
            return self._last_cleanup_ts_tnb
            
        new_cleanup_score = await self._add_to_zset_with_cleanup(
            key=key,
            value=value,
            score=score, # Timestamp is the score
            last_cleanup_score=self._last_cleanup_ts_tnb,
            cleanup_interval=self._timestamp_cleanup_interval, # Using timestamp interval
            max_items=self.retention_config.max_timestamps,    # Using max_timestamps
            connection_or_pipeline=conn_or_pipe
        )
        if pipeline is None: # Executed immediately
            self._last_cleanup_ts_tnb = new_cleanup_score
        return new_cleanup_score
        
    async def update_last_cleanup_scores(self, 
                                       block_data_score: Optional[int] = None, 
                                       ts_bnt_score: Optional[int] = None, 
                                       ts_tnb_score: Optional[int] = None):
        """Updates the internal last cleanup scores. Call after successful pipeline execution."""
        if block_data_score is not None:
            self._last_cleanup_block_data = block_data_score
        if ts_bnt_score is not None:
            self._last_cleanup_ts_bnt = ts_bnt_score
        if ts_tnb_score is not None:
            self._last_cleanup_ts_tnb = ts_tnb_score
        self._logger.debug(
            f"Updated last cleanup scores: block_data={self._last_cleanup_block_data}, "
            f"ts_bnt={self._last_cleanup_ts_bnt}, ts_tnb={self._last_cleanup_ts_tnb}"
        )

    async def get_zset_size(self, key: str) -> int:
        """Get the current size of a zset."""
        conn = await self.get_connection()
        if not conn: return 0
        return await conn.zcard(key)

    async def get_zset_range(self, key: str, start: int = 0, end: int = -1) -> list:
        """Get a range of values from a zset."""
        conn = await self.get_connection()
        if not conn: return []
        return await conn.zrange(key, start, end)

    async def close(self):
        """Cleanup resources. RedisPool handles actual connection closing."""
        self._logger.info("RedisDataManager close called. No explicit connection closing needed here as RedisPool manages it.")
        pass 


================================================
FILE: utils/redis/redis_conn.py
================================================
import redis.exceptions
from redis import asyncio as aioredis
from redis.asyncio.connection import ConnectionPool
from utils.logging import logger
from config.loader import get_core_config
from typing import Optional

class RedisPool:
    """Singleton Redis connection pool manager."""
    
    _instance: Optional['RedisPool'] = None
    _pool: Optional[aioredis.Redis] = None
    _logger = logger.bind(module='RedisPool')

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

    def __init__(self):
        if not hasattr(self, 'initialized'):
            self.settings = get_core_config()
            self.initialized = True

    @classmethod
    def get_pool(cls) -> aioredis.Redis:
        """Get or create Redis connection pool."""
        if cls._pool is None:
            instance = cls()
            redis_settings = instance.settings.redis
            
            # Construct Redis URL with credentials if present
            url = f"redis://"
            if redis_settings.password:
                url += f":{redis_settings.password}@"
            url += f"{redis_settings.host}:{redis_settings.port}/{redis_settings.db}"
            
            # Create connection pool with retry on ReadOnlyError
            pool = ConnectionPool.from_url(
                url=url,
                max_connections=100,  # Reasonable default for most use cases
                decode_responses=False,  # Keep raw bytes for blockchain data
                retry_on_error=[redis.exceptions.ReadOnlyError]
            )
            
            cls._pool = aioredis.Redis(
                connection_pool=pool,
                ssl=redis_settings.ssl
            )
            instance._logger.info("Redis connection pool initialized")
        
        return cls._pool

    @classmethod
    async def close(cls):
        """Close the Redis connection pool."""
        if cls._pool:
            await cls._pool.close()
            cls._pool = None
            cls._instance._logger.info("Redis connection pool closed")

# For backwards compatibility, provide the old function name
# but use the new pool implementation internally
async def get_aioredis_pool(pool_size: int = 100) -> aioredis.Redis:
    """Legacy function for getting a Redis pool. Uses the new RedisPool implementation.
    
    Args:
        pool_size (int): Maximum number of connections (default: 100)
    
    Returns:
        aioredis.Redis: Redis client instance with connection pooling
    """
    return await RedisPool.get_pool()


